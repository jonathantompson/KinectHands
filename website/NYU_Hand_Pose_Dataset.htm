<!DOCTYPE html>
<html>
<!-- CSS and template comes from Bootstrap -->
  <head>
    <script src="hand_files/jquery.js" type="text/javascript"></script>
    <script src="hand_files/blockui.js" type="text/javascript"></script>
    <script src="hand_files/highcharts.js"></script>
    <script src="hand_files/exporting.js"></script>

    <script src="hand_files/bootstrap.min.js"></script>
    <script src="hand_files/bootbox.min.js"></script>
    
    <title> NYU Hand Pose Dataset</title>
    	
    <link rel="stylesheet" type="text/css" href="hand_files/bootstrap.css">
  </head>
  <body>

    <!-- <div class="container" style="width:1268px;"> -->
    <div class="container" style="width:90%;" name="top_container">
      <h1><table border="0" width="100%"><tbody><tr><td><a href="http://cims.nyu.edu/~tompson/#">NYU Hand Pose Dataset</a></td><td align="right"><img width="300px" src="hand_files/logo_nyu.jpg"></td></tr></tbody></table></h1>
      
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <ul class="nav">
              <li class="active"><a class="menu_item" href="#overview">Overview</a></li>
              <li><a class="menu_item" href="#download">Download</a></li>
              <li><a class="menu_item" href="#references">References</a></li>
              <li><a class="menu_item" href="#contact">Contact</a></li>
            </ul>
          </div>
        </div>
      </div>
        
      <script type="text/javascript" charset="utf-8">
       $(document).ready(function(){
	 $(".span12").hide();
         $("#overview").show();
       });
      </script> 
      
      <div class="row" style="width:95%;">
	      <div class="span12" id="contact" style="width: 100%; display: none;">
	        <h3>Team</h3>
          <table><tbody><tr>
            <td style="border-right:solid 10px transparent;"><a href="http://cims.nyu.edu/~tompson/"><img src="hand_files/jonathan.jpg" style="height: 140px"></a></td>
            <td style="border-right:solid 10px transparent;"><a href="http://www.murphystein.com/"><img src="hand_files/murphy.jpg" style="height: 140px"></a></td>
            <td style="border-right:solid 10px transparent;"><a href="http://yann.lecun.com/"><img src="hand_files/yann.jpg" style="height: 140px"></a></td>
            <td style="border-right:solid 10px transparent;"><a href="http://mrl.nyu.edu/~perlin/"><img src="hand_files/ken.jpg" style="height: 140px"></a></td>
          </tr>
          <tr>
            <td align="center" style="font-size:16px"><a href="http://cims.nyu.edu/~tompson/">Jonathan<br>Tompson</a></td>
            <td align="center" style="font-size:16px"><a href="http://www.murphystein.com/">Murphy<br>Stein</a></td>
            <td align="center" style="font-size:16px"><a href="http://yann.lecun.com/">Yann<br>Lecun</a></td>
            <td align="center" style="font-size:16px"><a href="http://mrl.nyu.edu/~perlin/">Ken<br>Perlin</a></td>
          </tr></tbody></table>

          <br>
          <p><i class="icon-envelope icon-black"></i> For questions contact Jonathan Tompson: <i>tompson@cims.nyu.edu</i></p>

        </div>

        <div class="span12" id="references" style="width: 100%; display: none;">
          <h3>References</h3>
          <ul>
            <li>
              <p><a href="http://cims.nyu.edu/~tompson/others/TOG_2014_paper_PREPRINT.pdf"><strong>Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks</strong></a></p>
              <p>Jonathan Tompson, Murphy Stein, Yann Lecun and Ken Perlin. </p>
              <p>TOG'14 (Presented at SIGGRAPH'14)</p>
            </li>
          </ul>
        </div>

        <div class="span12" id="overview" style="width: 100%; display: block;">

          <img src="hand_files/sample.png" style="width: 100%" align="center">

          <h3>Overview</h3>
          <p>The NYU Hand pose dataset contains 8252 test-set and 72757 training-set frames of captured RGBD data with ground-truth hand-pose information.  For each frame, the RGBD data from 3 Kinects is provided: a frontal view and 2 side views. The training set contains samples from a single user only (Jonathan Tompson), while the test set contains samples from two users (Murphy Stein and Jonathan Tompson).  A synthetic re-creation (rendering) of the hand pose is also provided for each view.</p>
          <p>We also provide the predicted joint locations from our ConvNet (for the test-set) so you can compare performance.  Note: for real-time prediction we used only the depth image from Kinect 1.</p>
          <p>The source code to fit the hand-model to the depth frames here can be found <a href="https://github.com/jonathantompson/ModelFit"><strong>here</strong></a></p>
          <h3>Citing the dataset</h3>
          <pre><code>@article{tompson14tog,
  author = {Jonathan Tompson and Murphy Stein and Yann Lecun and Ken Perlin}
  title = {Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks,
  journal = {ACM Transactions on Graphics},
  year = {2014},
  month = {August},
  volume = {33}
}</code></pre>
          <h3>Publication</h3>
          <table>
            <tr>
              <td align="center"><a href="http://cims.nyu.edu/~tompson/others/TOG_2014_paper_PREPRINT.pdf"><img src="hand_files/tog_paper_thumbnail.jpg" style="height: 120px;"></a></td>
              <td>&nbsp;&nbsp;&nbsp;</rd>
              <td align="center"><a href="http://cims.nyu.edu/~tompson/others/paper_presentation_no_videos.pptx"><img src="hand_files/tog_ppt_thumbnail.jpg" style="height: 120px"></a></td>
            </tr>
            <tr>
              <td align="center" style="font-size:16px"><a href="http://cims.nyu.edu/~tompson/others/TOG_2014_paper_PREPRINT.pdf">TOG'14 paper</a></td>
              <td>&nbsp;</rd>
              <td align="center" style="font-size:16px"><a href="http://cims.nyu.edu/~tompson/others/paper_presentation_no_videos.pptx">SIGGRAPH'14 ppt</a></td>
            </tr>
          </table>
        </div>

        <div class="span12" id="download" style="width: 100%; display: block;">
          <h3>Download</h3>
          <p>You can download the dataset here:</p>

          <p><a href="http://blackbox.cs.nyu.edu/nyu_hand_dataset_v2.zip"><i class="icon-download icon-black"></i>nyu_hand_dataset_v2.zip (92 GB)</a></p>
      
          <h3>Dataset format</h3>
          <p>The top level directory is structured as follows:</p>
          <ul>
            <li><p><code>visualize_example.m</code> - Example script: loading and displaying one data sample</p></li>
            <li><p><code>evaluate_predictions.m</code> - Example script: displaying our detector's predicted coordinates and performance</p></li>
            <li>
              <p><code>test</code></p>
              <ul>
                <li><code>depth_&lt;k&gt;_&lt;f&gt;.png</code> - Test-set Depth frame <code>&lt;f&gt;</code> for <code>&lt;k&gt;</code> kinect.</li>
                <li><code>synthdepth_&lt;k&gt;_&lt;f&gt;.png</code> - Test-set Synthetic depth frame <code>&lt;f&gt;</code> for <code>&lt;k&gt;</code> kinect.</li>
                <li><code>rgb_&lt;k&gt;_&lt;f&gt;.png</code> - Test-set RGB frame <code>&lt;f&gt;</code> for <code>&lt;k&gt;</code> kinect.</li>
                <li><code>joint_data.mat</code> - Matlab data containing:
                  <ul>
                    <li><code>joint_names</code> - Cell of strings containing the names of the 36 key hand locations</li>
                    <li><code>joint_uvd</code> - 4D Tensor containing the UVD location of each joint in the test-set frames</li>
                    <li><code>joint_xyz</code> - 4D Tensor containing the XYZ location of each joint in the test-set frames</li>
                  </ul>
                </li>
                <li><code>test_predictions.mat</code> - Matlab data containing:
                  <ul>
                    <li><code>conv_joint_names</code> - Cell of string containing the names of locations tracked by the ConvNet</li>
                    <li><code>pred_joint_uvconf</code> - UV and confidence (likelihood) for each tracked joint</li>
                  </ul>
                </li>
              </ul>
            </li>
            <li>
              <p><code>train</code></p>
              <ul>
                <li><code>depth_&lt;k&gt;_&lt;f&gt;.png</code> - Training-set depth frame <code>&lt;f&gt;</code> for <code>&lt;k&gt;</code> kinect.</li>
                <li><code>synthdepth_&lt;k&gt;_&lt;f&gt;.png</code> - Training-set Synthetic depth frame <code>&lt;f&gt;</code> for <code>&lt;k&gt;</code> kinect.</li>
                <li><code>rgb_&lt;k&gt;_&lt;f&gt;.png</code> - Training-set RGB frame <code>&lt;f&gt;</code> for <code>&lt;k&gt;</code> kinect.</li>
                <li><code>joint_data.mat</code> - Matlab data containing:
                  <ul>
                    <li><code>joint_names</code> - Cell of strings containing the names of the 36 key hand locations</li>
                    <li><code>joint_uvd</code> - 4D Tensor containing the UVD location of each joint in the training-set frames</li>
                    <li><code>joint_xyz</code> - 4D Tensor containing the XYZ location of each joint in the training-set frames</li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul> 

          <p>Note: In each depth png file the top 8 bits of depth are packed into the green channel and the lower 8 bits into blue.</p>

        </div>
      </div>

      <script>
        <!--
        $(".menu_item").click(function(){
      $("ul.nav > li").removeClass("active");


      $(".span12").hide();
            theDiv = $(this).attr("href");

              $(theDiv).show();
        
            $(this).closest("li").toggleClass("active");
        });
      </script>
    </div>
  </body>
</html>
